{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of train with a KeypointRCNN_ResNet50_FPN pre-trained model\n",
    "\n",
    "In the code below, we are going to use the KeypointRCNN_ResNet50_FPN_Weights model pre-trained on the COCO dataset to train on our own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 1080, 1920] at entry 0 and [3, 1920, 1080] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mcollate_fn)\n\u001b[1;32m    103\u001b[0m \u001b[39m# Get a batch of images and annotations from the train dataloader\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m images, targets \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dataloader))\n\u001b[1;32m    106\u001b[0m \u001b[39m# Get the first image and its annotations from the batch\u001b[39;00m\n\u001b[1;32m    107\u001b[0m image \u001b[39m=\u001b[39m images[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtranspose((\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mcopy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[47], line 95\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     91\u001b[0m keypoints \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((keypoints, ones), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     93\u001b[0m keypoints \u001b[39m=\u001b[39m keypoints\u001b[39m.\u001b[39mview(keypoints\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m \u001b[39mreturn\u001b[39;00m  torch\u001b[39m.\u001b[39;49mstack(images),  torch\u001b[39m.\u001b[39mstack(annotations)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 1080, 1920] at entry 0 and [3, 1920, 1080] at entry 2"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import kornia as K\n",
    "from kornia import augmentation as A\n",
    "from kornia.augmentation import AugmentationSequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if cuda is avaliable, use it\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "train_images_path = 'dataset/train/images'\n",
    "train_annotations_path = 'dataset/train/annotations'\n",
    "test_images_path = 'dataset/test'\n",
    "test_annotations_path = 'dataset/test/annotations'\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_path, annotations_path, device):\n",
    "        self.images_path = images_path\n",
    "        self.annotations_path = annotations_path\n",
    "        self.device = device\n",
    "        self.image_filenames = [filename for filename in os.listdir(images_path) if filename.endswith('.jpg')]\n",
    "\n",
    "       # Declare an augmentation pipeline\n",
    "        self.transform =K.augmentation.AugmentationSequential(\n",
    "            K.augmentation.RandomVerticalFlip(), \n",
    "            K.augmentation.RandomHorizontalFlip(),\n",
    "            K.augmentation.RandomRotation(30),\n",
    "            data_keys=[\"input\", \"bbox\", \"keypoints\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_filenames[idx]\n",
    "        image = self.load_image(os.path.join(self.images_path, image_filename))\n",
    "        annotation_filename = os.path.join(self.annotations_path, image_filename[:-4] + '.json')\n",
    "        with open(annotation_filename, 'r') as f:\n",
    "            annotation = json.load(f)\n",
    "\n",
    "        target = []      \n",
    "        target.append({\n",
    "            'boxes': torch.tensor(annotation['bboxes']).float().to(self.device),\n",
    "            'labels': torch.as_tensor([1 for _ in annotation['bboxes']], dtype=torch.int64).to(self.device),\n",
    "            'keypoints': torch.tensor(np.array(annotation['keypoints'])[:,:,:2] ).float().to(self.device)\n",
    "        })\n",
    "\n",
    "        # transform image and keypoints\n",
    "        transformed = self.transform(image, target[0]['boxes'],  target[0]['keypoints'])\n",
    "\n",
    "        # create target tensor\n",
    "        target = torch.ones([transformed[1].shape[0],transformed[1].shape[1],3]).to(device)\n",
    "\n",
    "        # assign keypoints and add visibility\n",
    "        target[:,:,:2] = transformed[1]\n",
    "\n",
    "        return transformed[0].squeeze(0), target.reshape(transformed[1].shape[0] * transformed[1].shape[1],3)\n",
    "\n",
    "    def load_image(self, image_path: str)-> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Method to load image\n",
    "        params:\n",
    "            image_path: str = path of the image\n",
    "        return:\n",
    "            tensor: torch.Tensor = image tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # load image and convert to tensor\n",
    "        image: np.ndarray = cv2.imread(image_path)\n",
    "\n",
    "        # convert to tensor\n",
    "        image_tensor: torch.Tensor = K.image_to_tensor(image)\n",
    "\n",
    "        # bgr to rgb\n",
    "        image_tensor = K.color.bgr_to_rgb(image_tensor)\n",
    "\n",
    "        return K.enhance.normalize(image_tensor, torch.tensor(0.), torch.tensor(255.)).to(self.device)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, annotations = zip(*batch)\n",
    "\n",
    "    import torch\n",
    "\n",
    "    keypoints = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "    ones = torch.ones((keypoints.shape[0], 1))\n",
    "    keypoints = torch.cat((keypoints, ones), dim=1)\n",
    "\n",
    "    keypoints = keypoints.view(keypoints.shape[0], -1, 3)\n",
    "\n",
    "    return  torch.stack(images),  torch.stack(annotations)\n",
    "\n",
    "train_dataset = CustomDataset(train_images_path, train_annotations_path, device)\n",
    "test_dataset = CustomDataset(test_images_path, test_annotations_path, device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Get a batch of images and annotations from the train dataloader\n",
    "images, targets = next(iter(train_dataloader))\n",
    "\n",
    "# Get the first image and its annotations from the batch\n",
    "image = images[0].numpy().transpose((1, 2, 0)).copy().astype(np.uint8)\n",
    "annotations = targets[0]\n",
    "\n",
    "# Draw the bounding boxes and keypoints on the image using OpenCV\n",
    "for bbox, keypoints in zip(annotations['boxes'], annotations['keypoints']):\n",
    "    bbox = bbox.numpy().astype(int)\n",
    "    keypoints = keypoints.numpy().reshape(-1, 3).astype(int)\n",
    "    image = cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 3)\n",
    "    for x, y, v in keypoints:\n",
    "        if v > 0:\n",
    "            image = cv2.circle(image, (x, y), 10, (0, 0, 255), 3)\n",
    "\n",
    "# Convert the image to RGB and Show the image using Matplotlib \n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the custom dataset to load image tensor and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data augmentation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot one image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
