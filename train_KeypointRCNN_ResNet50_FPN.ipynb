{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of train with a KeypointRCNN_ResNet50_FPN pre-trained model\n",
    "\n",
    "In the code below, we are going to use the KeypointRCNN_ResNet50_FPN_Weights model pre-trained on the COCO dataset to train on our own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import kornia as K\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_path, annotations_path, use_augmentation, device, downsample_factor=1):\n",
    "        self.images_path = images_path\n",
    "        self.annotations_path = annotations_path\n",
    "        self.device = device\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.image_filenames = [filename for filename in os.listdir(images_path) if filename.endswith('.jpg')]\n",
    "\n",
    "        if use_augmentation:\n",
    "            # Declare an augmentation pipeline\n",
    "            self.transform =K.augmentation.AugmentationSequential(\n",
    "                K.augmentation.RandomVerticalFlip(), \n",
    "                K.augmentation.RandomHorizontalFlip(),\n",
    "                K.augmentation.RandomRotation(30),\n",
    "                K.augmentation.RandomBrightness(0.5, 1.5),\n",
    "                K.augmentation.RandomContrast(0.5, 1.5),\n",
    "                data_keys=[\"input\", \"bbox\", \"keypoints\"])\n",
    "        else:\n",
    "            self.transform = K.augmentation.AugmentationSequential(\n",
    "                K.augmentation.RandomRotation(0),\n",
    "                data_keys=[\"input\", \"bbox\", \"keypoints\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_filename = self.image_filenames[idx]\n",
    "        image_tensor = self.load_image(os.path.join(self.images_path, image_filename))\n",
    "        annotation_filename = os.path.join(self.annotations_path, image_filename[:-4] + '.json')\n",
    "\n",
    "        with open(annotation_filename, 'r') as f:\n",
    "            bbox_tensor, keypoint_tensor = self.convert_to_kornia_format(json.load(f))\n",
    "    \n",
    "        out_tensor = self.transform(image_tensor.float(), bbox_tensor.float(), keypoint_tensor.float())\n",
    "        \n",
    "        # plot augmentation to test it\n",
    "        if False:\n",
    "            img_out = self.plot_resulting_image(\n",
    "            out_tensor[0][0],\n",
    "            out_tensor[1].int(),\n",
    "            out_tensor[2].int(),\n",
    "            )\n",
    "            # plot the image\n",
    "            plt.imshow(K.tensor_to_image(image_tensor.mul(255).byte()).copy())\n",
    "            plt.show()\n",
    "            plt.imshow(img_out)\n",
    "            plt.show()\n",
    "\n",
    "        # get the torch format from kornia format\n",
    "        target = self.kornia_to_torch_format( out_tensor[1], out_tensor[2], idx)\n",
    "\n",
    "        # return tensors\n",
    "        return image_tensor, target\n",
    "\n",
    "    def load_image(self, image_path: str)-> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Method to load image\n",
    "        params:\n",
    "            image_path: str = path of the image\n",
    "        return:\n",
    "            tensor: torch.Tensor = image tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # load image and convert to tensor\n",
    "        image: np.ndarray = cv2.imread(image_path)\n",
    "\n",
    "        # convert to tensor\n",
    "        image_tensor: torch.Tensor = K.image_to_tensor(image)\n",
    "\n",
    "        # bgr to rgb\n",
    "        image_tensor = K.color.bgr_to_rgb(image_tensor)\n",
    "\n",
    "        return K.enhance.normalize(image_tensor, torch.tensor(0.), torch.tensor(255.)).to(self.device)\n",
    "    \n",
    "    def convert_to_kornia_format(self, data):\n",
    "        \"\"\"\n",
    "        Method to convert the bounding boxes and keypoints to the Kornia format\n",
    "        params:\n",
    "            data: dict = dictionary containing the bounding boxes and keypoints\n",
    "        return:\n",
    "            bbox_tensor: torch.Tensor = tensor containing the bounding boxes\n",
    "            keypoint_tensor: torch.Tensor = tensor containing the keypoints\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the bounding boxes and keypoints from the dictionary\n",
    "        bboxes = data['bboxes']\n",
    "        keypoints = data['keypoints']\n",
    "\n",
    "        # Convert the bounding boxes to the Kornia format\n",
    "        bbox_list = []\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            bbox_list.append([[x1, y1], [x2, y1], [x2, y2], [x1, y2]])\n",
    "        bbox_tensor = torch.tensor(bbox_list).unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Convert the keypoints to the Kornia format\n",
    "        keypoint_list = []\n",
    "        for kpts in keypoints:\n",
    "            for kpt in kpts:\n",
    "                x, y, _ = kpt\n",
    "                keypoint_list.append([x, y])\n",
    "        keypoint_tensor = torch.tensor(keypoint_list).unsqueeze(0).to(self.device)\n",
    "        return bbox_tensor, keypoint_tensor\n",
    "    \n",
    "    def plot_resulting_image(self, img, bbox, keypoints):\n",
    "        \"\"\"\n",
    "        Plot the resulting image with bounding boxes and keypoints.\n",
    "        params:\n",
    "            img: torch.Tensor = image tensor\n",
    "            bbox: torch.Tensor = bounding box tensor\n",
    "            keypoints: torch.Tensor = keypoints tensor\n",
    "        return:\n",
    "            img_draw = image with bounding boxes and keypoints\n",
    "        \"\"\"\n",
    "        img_array = K.tensor_to_image(img.mul(255).byte()).copy()\n",
    "        img_draw = cv2.polylines(img_array, bbox.reshape(-1, 4, 2).cpu().numpy(), isClosed=True, color=(255, 0, 0))\n",
    "        for k in keypoints[0]:\n",
    "            img_draw = cv2.circle(img_draw, tuple(k.cpu().numpy()[:2]), radius=6, color=(255, 0, 0), thickness=-1)\n",
    "        return img_draw\n",
    "\n",
    "    def kornia_to_torch_format(self, bbox_tensor, keypoint_tensor, idx,  labels=None):\n",
    "        \"\"\"\n",
    "        Convert bbox_tensor and keypoint_tensor in Kornia format to torch's expected format.\n",
    "        \n",
    "        Parameters:\n",
    "        - bbox_tensor (torch.Tensor): Bounding box tensor in Kornia format\n",
    "        - keypoint_tensor (torch.Tensor): Keypoint tensor in Kornia format\n",
    "        - idx = index of the image\n",
    "        - labels (list[int]): List of class labels for each bounding box. If None, default to label=1 for all boxes.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: A dictionary containing the following keys:\n",
    "            - boxes (torch.Tensor): Bounding box tensor in torch's expected format\n",
    "            - labels (torch.Tensor): Class label tensor\n",
    "            - image_id (torch.Tensor): Image ID tensor\n",
    "            - area (torch.Tensor): Area tensor\n",
    "            - iscrowd (torch.Tensor): IsCrowd tensor\n",
    "            - keypoints (torch.Tensor): Keypoint tensor in torch's expected format\n",
    "        \"\"\"\n",
    "        # Convert bbox_tensor from Kornia's format to torch's [x1, y1, x2, y2] format\n",
    "        boxes = torch.stack([bbox_tensor[0,:,0,0], bbox_tensor[0,:,0,1], bbox_tensor[0,:,2,0], bbox_tensor[0,:,2,1]], dim=1)\n",
    "        \n",
    "        # If labels aren't provided, assume a default label of 1 for all bounding boxes\n",
    "        if labels is None:\n",
    "            labels = torch.ones((bbox_tensor.shape[1],), dtype=torch.int64).to(self.device)\n",
    "        else:\n",
    "            labels = torch.tensor(labels, dtype=torch.int64).to(self.device)\n",
    "        \n",
    "        # Convert keypoint_tensor to the desired [x, y, visibility] format\n",
    "        keypoints = torch.zeros((bbox_tensor.shape[1], keypoint_tensor.shape[1]//bbox_tensor.shape[1], 3)).to(self.device)\n",
    "        for i in range(bbox_tensor.shape[1]):\n",
    "            keypoints[i, :, :2] = keypoint_tensor[0, i*2:(i+1)*2, :]\n",
    "            keypoints[i, :, 2] = 1  # setting visibility to 1\n",
    "        \n",
    "        return {\"boxes\": boxes, \"labels\": labels, \"keypoints\": keypoints, \"image_id \": torch.tensor([idx]).to(self.device), \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]), \"iscrowd\": torch.zeros((bbox_tensor.shape[1],), dtype=torch.int64).to(self.device)}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to format the batch in the desired manner.\n",
    "    \n",
    "    Parameters:\n",
    "    - batch (list): List of tuples where each tuple contains an image tensor and its associated target.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Tuple containing a tensor of images and a list of targets.\n",
    "    \"\"\"\n",
    "    # Separate images and targets in the batch\n",
    "    images, targets = zip(*batch)\n",
    "    \n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# if cuda is avaliable, use it\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "train_images_path = 'dataset/train/images'\n",
    "train_annotations_path = 'dataset/train/annotations'\n",
    "test_images_path = 'dataset/test/images'\n",
    "test_annotations_path = 'dataset/test/annotations'\n",
    "\n",
    "train_dataset = CustomDataset(train_images_path, train_annotations_path,use_augmentation = True, device = device)\n",
    "test_dataset = CustomDataset(test_images_path, test_annotations_path,  use_augmentation = False, device = device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Get a batch of images and annotations from the train dataloader\n",
    "images, targets = next(iter(train_dataloader))\n",
    "\n",
    "print(len(images))\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=2,\n",
    "                                                                   num_classes = 2,\n",
    "                                                                   trainable_backbone_layers=3)\n",
    "model.to(device).train()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some hyperparameters\n",
    "num_epochs = 300\n",
    "lr = 0.001\n",
    "\n",
    "# Create a directory with the current timestamp\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "save_dir = os.path.join(\"saved_models\", timestamp)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# select parameters to finetune\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(params, lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "best_loss = float('inf')  # Initialize with a high value\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for images, targets in tqdm(train_dataloader):\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        # Compute total loss\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += losses.item()\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(test_dataloader):\n",
    "\n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "            # Compute total loss\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += losses.item()\n",
    "            \n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Training Loss: {avg_train_loss}\")\n",
    "\n",
    "    # Compute average validation loss for the epoch\n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "    print(f\"Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    # Save the model if the validation loss improved\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        print(f\"Improved validation loss at epoch {epoch+1}. Saving model...\")\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f\"model_best_epoch_{epoch+1}.pth\"))\n",
    "    \n",
    "    # empty cuda cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Here the code to test the model on the test set is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of directories in saved_models\n",
    "dirs = os.listdir(\"saved_models\")\n",
    "\n",
    "# Sort the directories by creation time\n",
    "dirs = sorted(dirs, key=lambda x: os.path.getctime(os.path.join(\"saved_models\", x)))\n",
    "\n",
    "# Get the path of the most recent directory\n",
    "latest_dir = os.path.join(\"saved_models\", dirs[-1])\n",
    "\n",
    "# Get the path of the last saved model inside the most recent directory\n",
    "model_path = os.path.join(latest_dir,sorted(os.listdir(latest_dir))[-1])\n",
    "\n",
    "# load model \n",
    "model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   pretrained_backbone=True,\n",
    "                                                                   num_keypoints=2,\n",
    "                                                                   num_classes = 2,\n",
    "                                                                   trainable_backbone_layers=3)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval().to(device)\n",
    "\n",
    "# load image\n",
    "img_path = 'dataset/test/images/IMG_4913_JPG_jpg.rf.4f67c223e9cbf0ed07236bfe142aaaee.jpg'\n",
    "image = cv2.imread(img_path)\n",
    "\n",
    "# convert to tensor\n",
    "image_tensor: torch.Tensor = K.image_to_tensor(image).to(device)\n",
    "\n",
    "# bgr to rgb\n",
    "image_tensor = K.color.bgr_to_rgb(image_tensor)\n",
    "\n",
    "# normalize\n",
    "image_tensor = K.enhance.normalize(image_tensor, torch.tensor(0.), torch.tensor(255.)).to(device)\n",
    "\n",
    "# add batch dimension\n",
    "image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "# inference\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(image_tensor)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "keypoints = predictions[0]['keypoints'].detach().cpu().numpy()\n",
    "boxes = predictions[0]['boxes'].detach().cpu().numpy()\n",
    "\n",
    "for i in range(len(predictions[0]['scores'])):\n",
    "    if predictions[0]['scores'][i] > 0.5:\n",
    "        cv2.rectangle(output_image, (int(boxes[i][0]), int(boxes[i][1])), (int(boxes[i][2]), int(boxes[i][3])), (0, 255, 0), 2)\n",
    "        for j in range(len(keypoints[i])):\n",
    "            cv2.circle(output_image, (int(keypoints[i][j][0]), int(keypoints[i][j][1])), 2, (0, 0, 255), 2)\n",
    "# imshow\n",
    "plt.imshow(output_image)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
