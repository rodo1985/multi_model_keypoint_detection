{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of train with a KeypointRCNN_ResNet50_FPN pre-trained model\n",
    "\n",
    "In the code below, we are going to use the KeypointRCNN_ResNet50_FPN_Weights model pre-trained on the COCO dataset to train on our own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import kornia as K\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision.models.detection import KeypointRCNN_ResNet50_FPN_Weights\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_path, annotations_path, use_augmentation, device):\n",
    "        self.images_path = images_path\n",
    "        self.annotations_path = annotations_path\n",
    "        self.device = device\n",
    "        self.image_filenames = [filename for filename in os.listdir(images_path) if filename.endswith('.jpg')]\n",
    "\n",
    "        if use_augmentation:\n",
    "            # Declare an augmentation pipeline\n",
    "            self.transform =K.augmentation.AugmentationSequential(\n",
    "                K.augmentation.Normalize(torch.tensor([0.485, 0.456, 0.406]).to(device), torch.tensor([0.229, 0.224, 0.225]).to(device)),\n",
    "                K.augmentation.RandomVerticalFlip(), \n",
    "                K.augmentation.RandomHorizontalFlip(),\n",
    "                K.augmentation.RandomRotation(30),\n",
    "                data_keys=[\"input\", \"bbox\", \"keypoints\"])\n",
    "        else:\n",
    "            self.transform = K.augmentation.AugmentationSequential(\n",
    "                K.augmentation.RandomRotation(0),\n",
    "                data_keys=[\"input\", \"bbox\", \"keypoints\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_filename = self.image_filenames[idx]\n",
    "        image_tensor = self.load_image(os.path.join(self.images_path, image_filename))\n",
    "        annotation_filename = os.path.join(self.annotations_path, image_filename[:-4] + '.json')\n",
    "\n",
    "        with open(annotation_filename, 'r') as f:\n",
    "            bbox_tensor, keypoint_tensor = self.convert_to_kornia_format(json.load(f))\n",
    "    \n",
    "        out_tensor = self.transform(image_tensor.float(), bbox_tensor.float(), keypoint_tensor.float())\n",
    "        \n",
    "        # plot augmentation to test it\n",
    "        if False:\n",
    "            img_out = self.plot_resulting_image(\n",
    "            out_tensor[0][0],\n",
    "            out_tensor[1].int(),\n",
    "            out_tensor[2].int(),\n",
    "            )\n",
    "            # plot the image\n",
    "            plt.imshow(K.tensor_to_image(image_tensor.mul(255).byte()).copy())\n",
    "            plt.show()\n",
    "            plt.imshow(img_out)\n",
    "            plt.show()\n",
    "\n",
    "        # get the torch format from kornia format\n",
    "        target = self.kornia_to_torch_format( out_tensor[1], out_tensor[2])\n",
    "\n",
    "        # return tensors\n",
    "        return image_tensor, target\n",
    "\n",
    "    def load_image(self, image_path: str)-> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Method to load image\n",
    "        params:\n",
    "            image_path: str = path of the image\n",
    "        return:\n",
    "            tensor: torch.Tensor = image tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # load image and convert to tensor\n",
    "        image: np.ndarray = cv2.imread(image_path)\n",
    "\n",
    "        # convert to tensor\n",
    "        image_tensor: torch.Tensor = K.image_to_tensor(image)\n",
    "\n",
    "        # bgr to rgb\n",
    "        image_tensor = K.color.bgr_to_rgb(image_tensor)\n",
    "\n",
    "        return K.enhance.normalize(image_tensor, torch.tensor(0.), torch.tensor(255.)).to(self.device)\n",
    "    \n",
    "    def convert_to_kornia_format(self, data):\n",
    "        \"\"\"\n",
    "        Method to convert the bounding boxes and keypoints to the Kornia format\n",
    "        params:\n",
    "            data: dict = dictionary containing the bounding boxes and keypoints\n",
    "        return:\n",
    "            bbox_tensor: torch.Tensor = tensor containing the bounding boxes\n",
    "            keypoint_tensor: torch.Tensor = tensor containing the keypoints\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the bounding boxes and keypoints from the dictionary\n",
    "        bboxes = data['bboxes']\n",
    "        keypoints = data['keypoints']\n",
    "\n",
    "        # Convert the bounding boxes to the Kornia format\n",
    "        bbox_list = []\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            bbox_list.append([[x1, y1], [x2, y1], [x2, y2], [x1, y2]])\n",
    "        bbox_tensor = torch.tensor(bbox_list).unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Convert the keypoints to the Kornia format\n",
    "        keypoint_list = []\n",
    "        for kpts in keypoints:\n",
    "            for kpt in kpts:\n",
    "                x, y, _ = kpt\n",
    "                keypoint_list.append([x, y])\n",
    "        keypoint_tensor = torch.tensor(keypoint_list).unsqueeze(0).to(self.device)\n",
    "        return bbox_tensor, keypoint_tensor\n",
    "    \n",
    "    def plot_resulting_image(self, img, bbox, keypoints):\n",
    "        \"\"\"\n",
    "        Plot the resulting image with bounding boxes and keypoints.\n",
    "        params:\n",
    "            img: torch.Tensor = image tensor\n",
    "            bbox: torch.Tensor = bounding box tensor\n",
    "            keypoints: torch.Tensor = keypoints tensor\n",
    "        return:\n",
    "            img_draw = image with bounding boxes and keypoints\n",
    "        \"\"\"\n",
    "        img_array = K.tensor_to_image(img.mul(255).byte()).copy()\n",
    "        img_draw = cv2.polylines(img_array, bbox.reshape(-1, 4, 2).cpu().numpy(), isClosed=True, color=(255, 0, 0))\n",
    "        for k in keypoints[0]:\n",
    "            img_draw = cv2.circle(img_draw, tuple(k.cpu().numpy()[:2]), radius=6, color=(255, 0, 0), thickness=-1)\n",
    "        return img_draw\n",
    "\n",
    "    def kornia_to_torch_format(self, bbox_tensor, keypoint_tensor, labels=None):\n",
    "        \"\"\"\n",
    "        Convert bbox_tensor and keypoint_tensor in Kornia format to torch's expected format.\n",
    "        \n",
    "        Parameters:\n",
    "        - bbox_tensor (torch.Tensor): Bounding box tensor in Kornia format\n",
    "        - keypoint_tensor (torch.Tensor): Keypoint tensor in Kornia format\n",
    "        - labels (list[int]): List of class labels for each bounding box. If None, default to label=1 for all boxes.\n",
    "        \n",
    "        Returns:\n",
    "        - dict: A dictionary with 'boxes', 'labels', and 'keypoints' in the format expected by torch.\n",
    "        \"\"\"\n",
    "        # Convert bbox_tensor from Kornia's format to torch's [x1, y1, x2, y2] format\n",
    "        boxes = torch.stack([bbox_tensor[0,:,0,0], bbox_tensor[0,:,0,1], bbox_tensor[0,:,2,0], bbox_tensor[0,:,2,1]], dim=1)\n",
    "        \n",
    "        # If labels aren't provided, assume a default label of 1 for all bounding boxes\n",
    "        if labels is None:\n",
    "            labels = torch.ones((bbox_tensor.shape[1],), dtype=torch.int64).to(self.device)\n",
    "        else:\n",
    "            labels = torch.tensor(labels, dtype=torch.int64).to(self.device)\n",
    "        \n",
    "        # Convert keypoint_tensor to the desired [x, y, visibility] format\n",
    "        keypoints = torch.zeros((bbox_tensor.shape[1], keypoint_tensor.shape[1]//bbox_tensor.shape[1], 3)).to(self.device)\n",
    "        for i in range(bbox_tensor.shape[1]):\n",
    "            keypoints[i, :, :2] = keypoint_tensor[0, i*2:(i+1)*2, :]\n",
    "            keypoints[i, :, 2] = 1  # setting visibility to 1\n",
    "        \n",
    "        return {\"boxes\": boxes, \"labels\": labels, \"keypoints\": keypoints}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to format the batch in the desired manner.\n",
    "    \n",
    "    Parameters:\n",
    "    - batch (list): List of tuples where each tuple contains an image tensor and its associated target.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: Tuple containing a tensor of images and a list of targets.\n",
    "    \"\"\"\n",
    "    # Separate images and targets in the batch\n",
    "    images, targets = zip(*batch)\n",
    "    \n",
    "    # # Stack images into a single tensor\n",
    "    # images = torch.stack(images, 0)\n",
    "    \n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# if cuda is avaliable, use it\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "train_images_path = 'dataset/train/images'\n",
    "train_annotations_path = 'dataset/train/annotations'\n",
    "test_images_path = 'dataset/test/images'\n",
    "test_annotations_path = 'dataset/test/annotations'\n",
    "\n",
    "train_dataset = CustomDataset(train_images_path, train_annotations_path,use_augmentation = True, device = device)\n",
    "test_dataset = CustomDataset(test_images_path, test_annotations_path,  use_augmentation = False, device = device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 8, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Get a batch of images and annotations from the train dataloader\n",
    "images, targets = next(iter(test_dataloader))\n",
    "\n",
    "print(len(images))\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeypointRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(640, 672, 704, 736, 768, 800), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-3): 4 x Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "    )\n",
      "    (keypoint_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
      "    (keypoint_head): KeypointRCNNHeads(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "    )\n",
      "    (keypoint_predictor): KeypointRCNNPredictor(\n",
      "      (kps_score_lowres): ConvTranspose2d(512, 17, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.keypointrcnn_resnet50_fpn(weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "model.to(device).train()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:12<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Training Loss: 10.680099964141846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Validation Loss: 8.159403165181478\n",
      "Improved validation loss at epoch 1. Saving model...\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] Training Loss: 8.259827273232597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] Validation Loss: 8.162141799926758\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:10<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] Training Loss: 8.227414608001709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] Validation Loss: 8.136830965677897\n",
      "Improved validation loss at epoch 3. Saving model...\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] Training Loss: 8.241299492972237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] Validation Loss: 8.130262692769369\n",
      "Improved validation loss at epoch 4. Saving model...\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] Training Loss: 8.246586799621582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] Validation Loss: 8.201896667480469\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:10<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] Training Loss: 8.21510832650321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] Validation Loss: 8.176630020141602\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] Training Loss: 8.337576729910714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] Validation Loss: 8.224010467529297\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:10<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] Training Loss: 8.276910645621163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] Validation Loss: 8.12479559580485\n",
      "Improved validation loss at epoch 8. Saving model...\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:11<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] Training Loss: 8.269008840833392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] Validation Loss: 8.212945938110352\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 12/14 [00:09<00:01,  1.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[39m.\u001b[39mtrain()  \u001b[39m# Set model to training mode\u001b[39;00m\n\u001b[1;32m     25\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfor\u001b[39;00m images, targets \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m     27\u001b[0m     \n\u001b[1;32m     28\u001b[0m     \u001b[39m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     31\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(annotation_filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     30\u001b[0m     bbox_tensor, keypoint_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_to_kornia_format(json\u001b[39m.\u001b[39mload(f))\n\u001b[0;32m---> 32\u001b[0m out_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image_tensor\u001b[39m.\u001b[39;49mfloat(), bbox_tensor\u001b[39m.\u001b[39;49mfloat(), keypoint_tensor\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     34\u001b[0m \u001b[39m# plot augmentation to test it\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/kornia/augmentation/container/augment.py:349\u001b[0m, in \u001b[0;36mAugmentationSequential.forward\u001b[0;34m(self, params, data_keys, *args)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    348\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_submodule(param\u001b[39m.\u001b[39mname)\n\u001b[0;32m--> 349\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_op\u001b[39m.\u001b[39;49mtransform(  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    350\u001b[0m         \u001b[39m*\u001b[39;49moutputs, module\u001b[39m=\u001b[39;49mmodule, param\u001b[39m=\u001b[39;49mparam, extra_args\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextra_args\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    352\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    353\u001b[0m         \u001b[39m# Make sure we are unpacking a list whilst post-proc\u001b[39;00m\n\u001b[1;32m    354\u001b[0m         outputs \u001b[39m=\u001b[39m [outputs]\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/kornia/augmentation/container/ops.py:109\u001b[0m, in \u001b[0;36mAugmentationSequentialOps.transform\u001b[0;34m(self, module, param, extra_args, data_keys, *arg)\u001b[0m\n\u001b[1;32m    107\u001b[0m     op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_op(dcate)\n\u001b[1;32m    108\u001b[0m     extra_arg \u001b[39m=\u001b[39m extra_args[dcate] \u001b[39mif\u001b[39;00m dcate \u001b[39min\u001b[39;00m extra_args \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m--> 109\u001b[0m     outputs\u001b[39m.\u001b[39mappend(op\u001b[39m.\u001b[39;49mtransform(inp, module, param\u001b[39m=\u001b[39;49mparam, extra_args\u001b[39m=\u001b[39;49mextra_arg))\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(outputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    111\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/kornia/augmentation/container/ops.py:159\u001b[0m, in \u001b[0;36mInputSequentialOps.transform\u001b[0;34m(cls, input, module, param, extra_args)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39minput\u001b[39m: Tensor, module: Module, param: ParamItem, extra_args: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {}) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, (_AugmentationBase, K\u001b[39m.\u001b[39mMixAugmentationBaseV2)):\n\u001b[0;32m--> 159\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m, params\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_instance_module_param(param), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_args)\n\u001b[1;32m    160\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, (K\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mImageSequentialBase,)):\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39mtransform_inputs(\u001b[39minput\u001b[39m, params\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_sequential_module_param(param), extra_args\u001b[39m=\u001b[39mextra_args)\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/kornia/augmentation/base.py:210\u001b[0m, in \u001b[0;36m_BasicAugmentationBase.forward\u001b[0;34m(self, input, params, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     params[\u001b[39m'\u001b[39m\u001b[39mbatch_prob\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m tensor([\u001b[39mTrue\u001b[39;00m] \u001b[39m*\u001b[39m batch_shape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    208\u001b[0m params, flags \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_kwargs_to_params_and_flags(params, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflags, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 210\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_func(in_tensor, params, flags)\n\u001b[1;32m    211\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_output_tensor(output, input_shape) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeepdim \u001b[39melse\u001b[39;00m output\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/kornia/augmentation/_2d/base.py:124\u001b[0m, in \u001b[0;36mRigidAffineAugmentationBase2D.apply_func\u001b[0;34m(self, in_tensor, params, flags)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m flags \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     flags \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflags\n\u001b[0;32m--> 124\u001b[0m trans_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_transformation_matrix(in_tensor, params, flags)\n\u001b[1;32m    125\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_inputs(in_tensor, params, flags, trans_matrix)\n\u001b[1;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_matrix \u001b[39m=\u001b[39m trans_matrix\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/kornia/augmentation/_2d/base.py:80\u001b[0m, in \u001b[0;36mRigidAffineAugmentationBase2D.generate_transformation_matrix\u001b[0;34m(self, input, params, flags)\u001b[0m\n\u001b[1;32m     78\u001b[0m     trans_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midentity_matrix(in_tensor)\n\u001b[1;32m     79\u001b[0m \u001b[39melif\u001b[39;00m to_apply\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 80\u001b[0m     trans_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_transformation(in_tensor, params\u001b[39m=\u001b[39;49mparams, flags\u001b[39m=\u001b[39;49mflags)\n\u001b[1;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     trans_matrix_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midentity_matrix(in_tensor)\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/kornia/augmentation/_2d/geometric/rotation.py:81\u001b[0m, in \u001b[0;36mRandomRotation.compute_transformation\u001b[0;34m(self, input, params, flags)\u001b[0m\n\u001b[1;32m     78\u001b[0m angles: Tensor \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mdegrees\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39minput\u001b[39m)\n\u001b[1;32m     80\u001b[0m center: Tensor \u001b[39m=\u001b[39m _compute_tensor_center(\u001b[39minput\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m rotation_mat: Tensor \u001b[39m=\u001b[39m _compute_rotation_matrix(angles, center\u001b[39m.\u001b[39;49mexpand(angles\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     83\u001b[0m \u001b[39m# rotation_mat is B x 2 x 3 and we need a B x 3 x 3 matrix\u001b[39;00m\n\u001b[1;32m     84\u001b[0m trans_mat: Tensor \u001b[39m=\u001b[39m eye_like(\u001b[39m3\u001b[39m, \u001b[39minput\u001b[39m, shared_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/kornia/geometry/transform/affwarp.py:61\u001b[0m, in \u001b[0;36m_compute_rotation_matrix\u001b[0;34m(angle, center)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute a pure affine rotation matrix.\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m scale: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones_like(center)\n\u001b[0;32m---> 61\u001b[0m matrix: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m get_rotation_matrix2d(center, angle, scale)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m matrix\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/kornia/geometry/transform/imgwarp.py:459\u001b[0m, in \u001b[0;36mget_rotation_matrix2d\u001b[0;34m(center, angle, scale)\u001b[0m\n\u001b[1;32m    456\u001b[0m scale_m[:, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m scale[:, \u001b[39m1\u001b[39m]\n\u001b[1;32m    458\u001b[0m rotat_m \u001b[39m=\u001b[39m eye_like(\u001b[39m3\u001b[39m, center)\n\u001b[0;32m--> 459\u001b[0m rotat_m[:, :\u001b[39m2\u001b[39m, :\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m angle_to_rotation_matrix(angle)\n\u001b[1;32m    461\u001b[0m affine_m \u001b[39m=\u001b[39m shift_m \u001b[39m@\u001b[39m rotat_m \u001b[39m@\u001b[39m scale_m \u001b[39m@\u001b[39m shift_m_inv\n\u001b[1;32m    462\u001b[0m \u001b[39mreturn\u001b[39;00m affine_m[:, :\u001b[39m2\u001b[39m, :]\n",
      "File \u001b[0;32m~/projects/multi_model_keypoint_detection/venv/lib/python3.10/site-packages/kornia/geometry/conversions.py:973\u001b[0m, in \u001b[0;36mangle_to_rotation_matrix\u001b[0;34m(angle)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mangle_to_rotation_matrix\u001b[39m(angle: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    961\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Create a rotation matrix out of angles in degrees.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \n\u001b[1;32m    963\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[39m        >>> output = angle_to_rotation_matrix(input)  # Nx3x2x2\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m     ang_rad \u001b[39m=\u001b[39m deg2rad(angle)\n\u001b[1;32m    974\u001b[0m     cos_a: Tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(ang_rad)\n\u001b[1;32m    975\u001b[0m     sin_a: Tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msin(ang_rad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define some hyperparameters\n",
    "num_epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "# Create a directory with the current timestamp\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "save_dir = os.path.join(\"saved_models\", timestamp)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "best_loss = float('inf')  # Initialize with a high value\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for images, targets in tqdm(train_dataloader):\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        # Compute total loss\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += losses.item()\n",
    "    \n",
    "    # Compute average loss for the epoch\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss}\")\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(test_dataloader):\n",
    "\n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "            # Compute total loss\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += losses.item()\n",
    "\n",
    "    # Compute average validation loss for the epoch\n",
    "    avg_val_loss = val_loss / len(test_dataloader)\n",
    "    print(f\"Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    # Save the model if the validation loss improved\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        print(f\"Improved validation loss at epoch {epoch+1}. Saving model...\")\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f\"model_best_epoch_{epoch+1}.pth\"))\n",
    "    \n",
    "    # empty cuda cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
